# ğŸš€ Border Surveillance ETL Pipeline

An end-to-end **ETL (Extract, Transform, Load)** pipeline built using **Databricks Free Tier** and **Apache Spark**, following the **Medallion Architecture (Bronzeâ€“Silverâ€“Gold)** approach.

---

## ğŸ“Œ Project Overview

This project simulates a **border surveillance data processing system**, where raw sensor data is ingested, cleaned, transformed, and converted into analytics-ready datasets.

The project demonstrates real-world **data engineering concepts**, including ETL pipelines, data modeling, and scalable architecture.


---

## ğŸ› ï¸ Technologies Used

- **Databricks (Free Tier)**
- **Apache Spark (PySpark)**
- **Delta Lake**
- **Python**
- **Git & GitHub**

---

## ğŸ“‚ Project Structure


---

## ğŸ” Pipeline Description

### ğŸ¥‰ Bronze Layer
- Ingests raw CSV data
- Preserves original structure
- Acts as raw landing zone

### ğŸ¥ˆ Silver Layer
- Cleans invalid or missing values
- Applies business logic
- Prepares structured datasets

### ğŸ¥‡ Gold Layer
- Aggregates processed data
- Creates analytics-ready tables
- Supports reporting and insights

---

## ğŸ“Š Use Cases
- Border activity monitoring  
- Surveillance analytics  
- Pattern detection and reporting  

---

## ğŸš€ How to Run the Project
1. Upload CSV file to Databricks workspace  
2. Execute Bronze â†’ Silver â†’ Gold notebooks  
3. Validate output tables  
4. Analyze results  

---

## ğŸ¯ Key Learnings
- Building end-to-end ETL pipelines  
- Working with Databricks and PySpark  
- Applying Medallion Architecture  
- Handling structured data at scale  

---

## ğŸ‘¤ Author
**Rutuja Ghodke**  
Aspiring Data Engineer | Databricks | PySpark | SQL  

---

â­ *If you found this project helpful, feel free to star the repository!*  


